{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"How To: Dell Poweredge 1950 Gen. III What is the Project? Suppose, in a strictly hypothetical scenario, you purchased a Generation III Dell Poweredge from your local PC recycler ( who may have given you a 40% discount ). Now you have a 1U rack server, made in 2009, and you're wondering what to do with it. This is a totally opinionated guide to getting the absolute most of a Gen. III Poweredge. But Why? Since hardware of this vintage has long since been phased out of data-centers, parts are cheap . While it will never compete with modern hardware on a watts / dollar basis, when fully spec'd out these systems are roughly equivalent to an c5d.4xlarge AWS EC2 instance, which would run about \\$550 per month. If you add a decent GPU into the mix you can end up with something closer to a p3.2xlarge instance, which would set you back \\$2200 / month. If we assume power is about $0.15 per kilowatt hour, we are looking at a monthly power bill in the \\$30-50 dollar range. This means if you need a big compute / build node Poweredge 1950's actually make a decent option. The Draw Backs For reasons we will discuss in the hardware section these machines are LOUD. As in, non-trivial risk of hearing damage loud. If you live in a small space there is no way to adequately mask the sound, and if you live in a large space you need to make peace with a room that will be borderline inhospitable. However, if you plan on colo'ing your system this will not make much of a difference. These machines are also not particularly flexible when it comes to hardware. They were never designed for high power PCIe devices, and those will only accept graphics cards under duress. Modern servers that include auxillary 12 volt plugs and have 2U sized PCIe risers take to graphics cards and unusual PCIe hardware much, much more easily.","title":"How To: Dell Poweredge 1950 Gen. III"},{"location":"#how-to-dell-poweredge-1950-gen-iii","text":"","title":"How To: Dell Poweredge 1950 Gen. III"},{"location":"#what-is-the-project","text":"Suppose, in a strictly hypothetical scenario, you purchased a Generation III Dell Poweredge from your local PC recycler ( who may have given you a 40% discount ). Now you have a 1U rack server, made in 2009, and you're wondering what to do with it. This is a totally opinionated guide to getting the absolute most of a Gen. III Poweredge.","title":"What is the Project?"},{"location":"#but-why","text":"Since hardware of this vintage has long since been phased out of data-centers, parts are cheap . While it will never compete with modern hardware on a watts / dollar basis, when fully spec'd out these systems are roughly equivalent to an c5d.4xlarge AWS EC2 instance, which would run about \\$550 per month. If you add a decent GPU into the mix you can end up with something closer to a p3.2xlarge instance, which would set you back \\$2200 / month. If we assume power is about $0.15 per kilowatt hour, we are looking at a monthly power bill in the \\$30-50 dollar range. This means if you need a big compute / build node Poweredge 1950's actually make a decent option.","title":"But Why?"},{"location":"#the-draw-backs","text":"For reasons we will discuss in the hardware section these machines are LOUD. As in, non-trivial risk of hearing damage loud. If you live in a small space there is no way to adequately mask the sound, and if you live in a large space you need to make peace with a room that will be borderline inhospitable. However, if you plan on colo'ing your system this will not make much of a difference. These machines are also not particularly flexible when it comes to hardware. They were never designed for high power PCIe devices, and those will only accept graphics cards under duress. Modern servers that include auxillary 12 volt plugs and have 2U sized PCIe risers take to graphics cards and unusual PCIe hardware much, much more easily.","title":"The Draw Backs"},{"location":"hardware/","text":"Hardware Processor The Poweredge 1950 comes equipped with 2 LGA771 processor slots. The best processor for this socket, based on PassMark score, seems to be the Intel Xeon X5450. This is a quad-core, none hyperthreaded processor that runs at 3.00 GHz. Caveats X5450's use a lot of power, with a TDP of 120 Watts each! A machine equipped with two X5450's, 64 GB of RAM and 4 2.5\" HDDs will draw over 300 Watts at idle . This can be a particular concern if you plan on colo'ing a unit and pay by the amp-hour, or have an amperage limit. RAM Up to 64 GB of DDR2 FB-DIMM ( Fully Buffered ) Ram, as 8 x 8 GB sticks, is supported. Any brand of memory should suffice, but A-Tech produces some of the cheapest 64 GB RAM bundles. Caveats Almost any after market memory suffers from a fairly serious issue. The Base Board Management Controller ( BMC ), the system that controls low-level devices such as the power supplies, fans and so forth, will not be able to read the temperature sensor integrated into each RAM stick. This will result in the cooling fans running much faster than usual under normal conditions -- for example idle conditions will produce speeds around 9-10K RPM instead of about 3-4.5K. As a result the server will be very, very loud. While there are ways to somehwhat control the BMC via Dell provided tools and custom BMC image flashing, none of these methods allow the fan speed threshold to be controlled. Storage Stock Hard Drive Bays The Poweredge 1950 could either be optioned with two 3.5\" quick-swap HDD bays or 4 2.5\" bays. These drives are connected to the back-place and the integrated RAID controller An extra SATA port Generation I and II versions of the Poweredge 1950 used an IDE bus to control the integrated optical drive. Generation III models added a lone SATA I bus to the main board, which was designed to replace the only IDE bus and control a SATA optical drive. Gen. III main power lead also had a small break-out cable to supply the 5 volts required by the optical drive. As fate would have it, the SATA bus and 5 volt power are exactly what is required to run a 2.5\" HDD. By using an \"Optical drive to HDD adapter\", such as this one the optical drive can be swapped out for an additional 2.5\" HDD. However this drive will not be accessible by the RAID controller and cannot be assigned to a RAID array with the main drives that connect to the back-plane. An extra USB port Generation III models of the Poweredge 1950 included a very curiously positioned USB port in the middle of the main board. Apparently this was supposed to be used for cryptography hardware keys -- fortunately it is just a normal USB 2.0 port. This allows for some fairly inserting possibilities. For example, with some clever cable routing, a USB-to-SATA converter and a 2.5\" HDD PCIe slot adapter an extra HDD can be added to the system, assuming there are empty PCIe slots available. There is also just enough space for a small USB thumb drive, An extra SAS port The integrated RAID controller comes equipped with two SAS ports, only one of which is used by the back-plane. This is probably due to the RAID controller being shared between the Poweredge 1950 and 2950, a unit that could support many more HDDs. By using a SAS-to-SATA break-out cable, like this one up to 4 additional HDDs can be attached to RAID controller. Of course there is no additional power or space available inside the server case, but if an external drive enclosure and power supply is used 4 HDDs can be provisioned by the RAID controller. Using Graphics Cards The Poweredge 1950 was produced in an era before GPGPU computing was in vogue and was not built with high-power use PCIe devices in mind. However, with enough work GPUs can be used PCIe Slots Available The Poweredge 1950 can be equipped with either two PCI-X risers, or two PCIe 2.0 8x risers. Note that these cannot be mixed and matched, it must be either two PCI-X risers, or two PCIe risers. Since there are a very limited number of PCI-X graphics cards available, none of which are any good, for GPU use the two PCIe risers will are almost always used. Caveats The PCIe 2.0 standard dictates that PCIe slots should provide 25 watts of power to standard devices and up to 75 watts to devices identified as \"high power\" such as graphics cards. However, it seems that the 8x PCIe slots do not comply with this part of the standard and will only supply up to 25 watts to any device. Using a Graphics Card as a VGA Device In order to use graphics cards as VGA devices, which is required for graphics output and OpenCL/CUDA device use, the iDRAC remote management card will need to be removed. In general the system will boot just fine with the iDRAC in place, but any attached graphics cards will not show up as valid VGA devices on boot. By removing the iDRAC and resetting the BIOS graphics cards can be used as VGA devices, as per usual. Using Graphics Cards that Require = 25 Watts Graphics cards that draw less than 25 watts will, generally not run afoul of power draw issues. However not every card will be recognized by the BIOS, and there doesn't appear to be any way to tell a prior which cards will work. Using Graphics Cards that Require 25 Watts Any card that is rated at 75 watt power usage will generally cause a POST failure with an PCIe riser error, assuming only power from the PCIe slot is used. However, if 12 volt power is supplied from an external source the card will usually work as intended. As a word of caution a power supplied to the card should always use the same ground reference. That is, using the 12 volt power from the PCIe rail and auxiliary power from an external power supply is not recommended . Power should either be supplied from the 12 volt sources available on the main board ( or the two power supplies ) or entirely from an external power supplies, as is the case when using a PCIe extender and external power supply. Resetting the BIOS Any time the iDRAC card is removed from or added to the system the BIOS will need to be reset or the system will hang on POST. The order to reset the BIOS the NVRAM_CLR jumper will need to be set to the \"clear\" position and the system will need to be boot cycled. A page from the maintenance manual for setting the NVRAM_CLR jumper is shown below. List of Graphics Cards Known to Work PNY GeForce GT 720 1GB DDR3 Sapphire Nitro Radeon R7 360 List of Graphics Cards Known NOT to Work Nvidia GT 1050(?)","title":"Hardware"},{"location":"hardware/#hardware","text":"","title":"Hardware"},{"location":"hardware/#processor","text":"The Poweredge 1950 comes equipped with 2 LGA771 processor slots. The best processor for this socket, based on PassMark score, seems to be the Intel Xeon X5450. This is a quad-core, none hyperthreaded processor that runs at 3.00 GHz.","title":"Processor"},{"location":"hardware/#caveats","text":"X5450's use a lot of power, with a TDP of 120 Watts each! A machine equipped with two X5450's, 64 GB of RAM and 4 2.5\" HDDs will draw over 300 Watts at idle . This can be a particular concern if you plan on colo'ing a unit and pay by the amp-hour, or have an amperage limit.","title":"Caveats"},{"location":"hardware/#ram","text":"Up to 64 GB of DDR2 FB-DIMM ( Fully Buffered ) Ram, as 8 x 8 GB sticks, is supported. Any brand of memory should suffice, but A-Tech produces some of the cheapest 64 GB RAM bundles.","title":"RAM"},{"location":"hardware/#caveats_1","text":"Almost any after market memory suffers from a fairly serious issue. The Base Board Management Controller ( BMC ), the system that controls low-level devices such as the power supplies, fans and so forth, will not be able to read the temperature sensor integrated into each RAM stick. This will result in the cooling fans running much faster than usual under normal conditions -- for example idle conditions will produce speeds around 9-10K RPM instead of about 3-4.5K. As a result the server will be very, very loud. While there are ways to somehwhat control the BMC via Dell provided tools and custom BMC image flashing, none of these methods allow the fan speed threshold to be controlled.","title":"Caveats"},{"location":"hardware/#storage","text":"","title":"Storage"},{"location":"hardware/#stock-hard-drive-bays","text":"The Poweredge 1950 could either be optioned with two 3.5\" quick-swap HDD bays or 4 2.5\" bays. These drives are connected to the back-place and the integrated RAID controller","title":"Stock Hard Drive Bays"},{"location":"hardware/#an-extra-sata-port","text":"Generation I and II versions of the Poweredge 1950 used an IDE bus to control the integrated optical drive. Generation III models added a lone SATA I bus to the main board, which was designed to replace the only IDE bus and control a SATA optical drive. Gen. III main power lead also had a small break-out cable to supply the 5 volts required by the optical drive. As fate would have it, the SATA bus and 5 volt power are exactly what is required to run a 2.5\" HDD. By using an \"Optical drive to HDD adapter\", such as this one the optical drive can be swapped out for an additional 2.5\" HDD. However this drive will not be accessible by the RAID controller and cannot be assigned to a RAID array with the main drives that connect to the back-plane.","title":"An extra SATA port"},{"location":"hardware/#an-extra-usb-port","text":"Generation III models of the Poweredge 1950 included a very curiously positioned USB port in the middle of the main board. Apparently this was supposed to be used for cryptography hardware keys -- fortunately it is just a normal USB 2.0 port. This allows for some fairly inserting possibilities. For example, with some clever cable routing, a USB-to-SATA converter and a 2.5\" HDD PCIe slot adapter an extra HDD can be added to the system, assuming there are empty PCIe slots available. There is also just enough space for a small USB thumb drive,","title":"An extra USB port"},{"location":"hardware/#an-extra-sas-port","text":"The integrated RAID controller comes equipped with two SAS ports, only one of which is used by the back-plane. This is probably due to the RAID controller being shared between the Poweredge 1950 and 2950, a unit that could support many more HDDs. By using a SAS-to-SATA break-out cable, like this one up to 4 additional HDDs can be attached to RAID controller. Of course there is no additional power or space available inside the server case, but if an external drive enclosure and power supply is used 4 HDDs can be provisioned by the RAID controller.","title":"An extra SAS port"},{"location":"hardware/#using-graphics-cards","text":"The Poweredge 1950 was produced in an era before GPGPU computing was in vogue and was not built with high-power use PCIe devices in mind. However, with enough work GPUs can be used","title":"Using Graphics Cards"},{"location":"hardware/#pcie-slots-available","text":"The Poweredge 1950 can be equipped with either two PCI-X risers, or two PCIe 2.0 8x risers. Note that these cannot be mixed and matched, it must be either two PCI-X risers, or two PCIe risers. Since there are a very limited number of PCI-X graphics cards available, none of which are any good, for GPU use the two PCIe risers will are almost always used.","title":"PCIe Slots Available"},{"location":"hardware/#caveats_2","text":"The PCIe 2.0 standard dictates that PCIe slots should provide 25 watts of power to standard devices and up to 75 watts to devices identified as \"high power\" such as graphics cards. However, it seems that the 8x PCIe slots do not comply with this part of the standard and will only supply up to 25 watts to any device.","title":"Caveats"},{"location":"hardware/#using-a-graphics-card-as-a-vga-device","text":"In order to use graphics cards as VGA devices, which is required for graphics output and OpenCL/CUDA device use, the iDRAC remote management card will need to be removed. In general the system will boot just fine with the iDRAC in place, but any attached graphics cards will not show up as valid VGA devices on boot. By removing the iDRAC and resetting the BIOS graphics cards can be used as VGA devices, as per usual.","title":"Using a Graphics Card as a VGA Device"},{"location":"hardware/#using-graphics-cards-that-require-25-watts","text":"Graphics cards that draw less than 25 watts will, generally not run afoul of power draw issues. However not every card will be recognized by the BIOS, and there doesn't appear to be any way to tell a prior which cards will work.","title":"Using Graphics Cards that Require &lt;= 25 Watts"},{"location":"hardware/#using-graphics-cards-that-require-25-watts_1","text":"Any card that is rated at 75 watt power usage will generally cause a POST failure with an PCIe riser error, assuming only power from the PCIe slot is used. However, if 12 volt power is supplied from an external source the card will usually work as intended. As a word of caution a power supplied to the card should always use the same ground reference. That is, using the 12 volt power from the PCIe rail and auxiliary power from an external power supply is not recommended . Power should either be supplied from the 12 volt sources available on the main board ( or the two power supplies ) or entirely from an external power supplies, as is the case when using a PCIe extender and external power supply.","title":"Using Graphics Cards that Require &gt; 25 Watts"},{"location":"hardware/#resetting-the-bios","text":"Any time the iDRAC card is removed from or added to the system the BIOS will need to be reset or the system will hang on POST. The order to reset the BIOS the NVRAM_CLR jumper will need to be set to the \"clear\" position and the system will need to be boot cycled. A page from the maintenance manual for setting the NVRAM_CLR jumper is shown below.","title":"Resetting the BIOS"},{"location":"hardware/#list-of-graphics-cards-known-to-work","text":"PNY GeForce GT 720 1GB DDR3 Sapphire Nitro Radeon R7 360","title":"List of Graphics Cards Known to Work"},{"location":"hardware/#list-of-graphics-cards-known-not-to-work","text":"Nvidia GT 1050(?)","title":"List of Graphics Cards Known NOT to Work"},{"location":"hypervisor/","text":"Hosting a Hypervisor Motivation So you have a big, fairly high-tech ( for 2009 ) 1U rack server set-up -- now what? You could run one gigantic bare-metal single OS, but that would almost certainly be inflexible and insecure. A more flexible approach, that is still fairly close to metal, is to run a hypervisor, like Xen or KVM . Installing a Base Image We will be using Debian Linux as our base image, for entirely preferential reasons. The rest of the guide will only apply to Debian systems but should provide a good template for other systems as well. Setting up KVM Install KVM and Supporting Utilities sudo apt-get install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils Install Virt-Mananger While not strictly necessary virt-manager provides a GUI for provisioning and monitoring VMs. sudo apt-get install virt-manager Set up a Network Bridge In order for any of our VMs to have network access we need to set up a network bridge to our host system. There are quite a few ways to do this, but one of the most straight-forward ways is to edit /etc/network/interfaces and set up a network bridge. Note that eth0 in the example files is just the name of the Ethernet adapter. The Poweredge 1950 has two NICs built-in which are labeled eth0 and eth1 so we could have just as easily used eth1 . Further note that Ethernet adapter names are not guaranteed to be consistent across OSes and hardware. If this guide is being adapted to other hardware of systems the name will need to be resolved and eth0 will need to be replaced in the interface file. To list all network interface devices, simply use ls /sys/class/net . Public Virtual Bridge In general not only want our VMs to be able to access network resources, but we also want to be able to access them without needing to go through the host machine. In order to do this we need a public bridge, which will expose any guest VMs on as machines on the same network as the host. Below is an example interfaces that will set-up a public bridge. Whenever we modify the /etc/network/interfaces file we will need to reboot the host machine or run /etc/init.d/networking restart . # /etc/network/interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo br0 iface lo inet loopback iface eth0 inet manual iface br0 inet dhcp bridge_ports eth0 bridge_stp off bridge_maxwait 0 bridge_fd 0 Set up PCIe Pass-through","title":"Hosting a Hypervisor"},{"location":"hypervisor/#hosting-a-hypervisor","text":"","title":"Hosting a Hypervisor"},{"location":"hypervisor/#motivation","text":"So you have a big, fairly high-tech ( for 2009 ) 1U rack server set-up -- now what? You could run one gigantic bare-metal single OS, but that would almost certainly be inflexible and insecure. A more flexible approach, that is still fairly close to metal, is to run a hypervisor, like Xen or KVM .","title":"Motivation"},{"location":"hypervisor/#installing-a-base-image","text":"We will be using Debian Linux as our base image, for entirely preferential reasons. The rest of the guide will only apply to Debian systems but should provide a good template for other systems as well.","title":"Installing a Base Image"},{"location":"hypervisor/#setting-up-kvm","text":"","title":"Setting up KVM"},{"location":"hypervisor/#install-kvm-and-supporting-utilities","text":"sudo apt-get install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils","title":"Install KVM and Supporting Utilities"},{"location":"hypervisor/#install-virt-mananger","text":"While not strictly necessary virt-manager provides a GUI for provisioning and monitoring VMs. sudo apt-get install virt-manager","title":"Install Virt-Mananger"},{"location":"hypervisor/#set-up-a-network-bridge","text":"In order for any of our VMs to have network access we need to set up a network bridge to our host system. There are quite a few ways to do this, but one of the most straight-forward ways is to edit /etc/network/interfaces and set up a network bridge. Note that eth0 in the example files is just the name of the Ethernet adapter. The Poweredge 1950 has two NICs built-in which are labeled eth0 and eth1 so we could have just as easily used eth1 . Further note that Ethernet adapter names are not guaranteed to be consistent across OSes and hardware. If this guide is being adapted to other hardware of systems the name will need to be resolved and eth0 will need to be replaced in the interface file. To list all network interface devices, simply use ls /sys/class/net .","title":"Set up a Network Bridge"},{"location":"hypervisor/#public-virtual-bridge","text":"In general not only want our VMs to be able to access network resources, but we also want to be able to access them without needing to go through the host machine. In order to do this we need a public bridge, which will expose any guest VMs on as machines on the same network as the host. Below is an example interfaces that will set-up a public bridge. Whenever we modify the /etc/network/interfaces file we will need to reboot the host machine or run /etc/init.d/networking restart . # /etc/network/interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo br0 iface lo inet loopback iface eth0 inet manual iface br0 inet dhcp bridge_ports eth0 bridge_stp off bridge_maxwait 0 bridge_fd 0","title":"Public Virtual Bridge"},{"location":"hypervisor/#set-up-pcie-pass-through","text":"","title":"Set up PCIe Pass-through"}]}